{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/benjismith/somewhere-ml/blob/main/VQGAN_MultiJob.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VA1PHoJrRiK9"
      },
      "outputs": [],
      "source": [
        "#@title MIT License\n",
        "# \n",
        "\n",
        "# Contributors:\n",
        "# \n",
        "# Copyright (c) 2021 [Katherine Crowson](https://github.com/crowsonkb)\n",
        "# Copyright (c) 2021 [Justin Bennington](https://github.com/justin-bennington)\n",
        "# Copyright (c) 2021 [Benji Smith](https://github.com/benjismith)\n",
        "\n",
        "# Forked from:\n",
        "# https://github.com/justin-bennington/S2ML-Art-Generator/blob/main/S2ML_Art_Generator.ipynb\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "5wCNUGHyfyPO"
      },
      "outputs": [],
      "source": [
        "#@markdown What GPU am I using?\n",
        "\n",
        "#@markdown V100 > P100 > everything else\n",
        "\n",
        "!nvidia-smi --query-gpu=gpu_name,gpu_bus_id,vbios_version --format=csv\n",
        "gpu_name = !nvidia-smi --query-gpu=gpu_name, --format=csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@markdown Setup AWS Credentials and S3 Bucket\n",
        "\n",
        "AWS_ACCESS_KEY_ID = \"\"#@param {type:\"string\"}\n",
        "AWS_SECRET_ACCESS_KEY = \"\"#@param {type:\"string\"}\n",
        "S3_BUCKET_NAME = \"\"#@param {type:\"string\"}\n",
        "S3_JOBS_JSON_PATH = \"\"#@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wSfISAhyPmyp"
      },
      "outputs": [],
      "source": [
        "# @title Setup, Installing Libraries\n",
        "# @markdown This cell might take some time due to installing several libraries.\n",
        "import os \n",
        "!nvidia-smi\n",
        "print(\"Downloading CLIP...\")\n",
        "!git clone https://github.com/openai/CLIP                 &> /dev/null\n",
        "!pip install -e ./CLIP                                    &> /dev/null\n",
        "\n",
        "print(\"Installing Python Libraries for AI\")\n",
        "!git clone https://github.com/CompVis/taming-transformers &> /dev/null\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning  &> /dev/null\n",
        "!pip install kornia                                       &> /dev/null\n",
        "!pip install einops                                       &> /dev/null\n",
        "\n",
        "print(\"Installing transformers library...\")\n",
        "!pip install transformers                                 &> /dev/null\n",
        " \n",
        "print(\"Installing libraries for managing metadata...\")\n",
        "!pip install boto3                                        &> /dev/null\n",
        "!pip install requests                                     &> /dev/null\n",
        "!apt install exempi                                       &> /dev/null\n",
        "!pip install python-xmp-toolkit                           &> /dev/null\n",
        "!pip install imgtag                                       &> /dev/null\n",
        "!pip install pillow==7.1.2                                &> /dev/null\n",
        "!pip install taming-transformers                          &> /dev/null\n",
        "\n",
        "print(\"Installing ffmpeg for creating videos...\")\n",
        "!pip install imageio-ffmpeg &> /dev/null\n",
        "\n",
        "!pip freeze > requirements.txt\n",
        "print(\"Installation finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FhhdWrSxQhwg"
      },
      "outputs": [],
      "source": [
        "#@title Selection of models to download\n",
        "#@markdown Ensure you select a model you've downloaded in the parameters block\n",
        "\n",
        "imagenet_1024 = False #@param {type:\"boolean\"}\n",
        "imagenet_16384 = True #@param {type:\"boolean\"}\n",
        "coco = False #@param {type:\"boolean\"}\n",
        "faceshq = False #@param {type:\"boolean\"}\n",
        "wikiart_1024 = False #@param {type:\"boolean\"}\n",
        "wikiart_16384 = False #@param {type:\"boolean\"}\n",
        "sflckr = False #@param {type:\"boolean\"}\n",
        "\n",
        "abs_root_path = \"/content\"\n",
        "if not os.path.exists(abs_root_path):\n",
        "    os.mkdir(abs_root_path)\n",
        "\n",
        "os.chdir(abs_root_path)\n",
        "\n",
        "if not os.path.exists(abs_root_path + \"/models\"):\n",
        "  os.mkdir(abs_root_path + \"/models\")\n",
        "\n",
        "os.chdir(abs_root_path + \"/models\")\n",
        "\n",
        "if imagenet_1024:\n",
        "  !curl -L -o vqgan_imagenet_f16_1024.ckpt -C - 'https://heibox.uni-heidelberg.de/f/140747ba53464f49b476/?dl=1' \n",
        "  !curl -L -o vqgan_imagenet_f16_1024.yaml -C - 'https://heibox.uni-heidelberg.de/f/6ecf2af6c658432c8298/?dl=1' \n",
        "  \n",
        "if imagenet_16384:\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.ckpt -C - 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1'\n",
        "  !curl -L -o vqgan_imagenet_f16_16384.yaml -C - 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1'\n",
        "\n",
        "if coco:\n",
        "  !curl -L -o coco.yaml -C - 'https://dl.nmkd.de/ai/clip/coco/coco.yaml' #COCO\n",
        "  !curl -L -o coco.ckpt -C - 'https://dl.nmkd.de/ai/clip/coco/coco.ckpt' #COCO\n",
        "\n",
        "if faceshq:\n",
        "  !curl -L -o faceshq.yaml -C - 'https://drive.google.com/uc?export=download&id=1fHwGx_hnBtC8nsq7hesJvs-Klv-P0gzT' #FacesHQ\n",
        "  !curl -L -o faceshq.ckpt -C - 'https://app.koofr.net/content/links/a04deec9-0c59-4673-8b37-3d696fe63a5d/files/get/last.ckpt?path=%2F2020-11-13T21-41-45_faceshq_transformer%2Fcheckpoints%2Flast.ckpt' #FacesHQ\n",
        "\n",
        "if wikiart_1024: \n",
        "  !curl -L -o wikiart_1024.yaml -C - 'http://mirror.io.community/blob/vqgan/wikiart.yaml' #WikiArt 1024\n",
        "  !curl -L -o wikiart_1024.ckpt -C - 'http://mirror.io.community/blob/vqgan/wikiart.ckpt' #WikiArt 1024\n",
        "\n",
        "if wikiart_16384: \n",
        "  !curl -L -o wikiart_16384.ckpt -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.ckpt'\n",
        "  !curl -L -o wikiart_16384.yaml -C - 'http://eaidata.bmk.sh/data/Wikiart_16384/wikiart_f16_16384_8145600.yaml'\n",
        "\n",
        "if sflckr:\n",
        "  !curl -L -o sflckr.yaml -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fconfigs%2F2020-11-09T13-31-51-project.yaml&dl=1' #S-FLCKR\n",
        "  !curl -L -o sflckr.ckpt -C - 'https://heibox.uni-heidelberg.de/d/73487ab6e5314cb5adba/files/?p=%2Fcheckpoints%2Flast.ckpt&dl=1' #S-FLCKR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EXMSuW2EQWsd"
      },
      "outputs": [],
      "source": [
        "# @title Load libraries and definitions\n",
        "print(abs_root_path)\n",
        "os.chdir(abs_root_path)\n",
        "!pwd\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import io\n",
        "import sys\n",
        " \n",
        "sys.path.append('./taming-transformers')\n",
        "from IPython import display\n",
        "from base64 import b64encode\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        " \n",
        "from CLIP import clip\n",
        "import kornia.augmentation as K\n",
        "import numpy as np\n",
        "import imageio\n",
        "from PIL import ImageFile, Image\n",
        "from imgtag import ImgTag    # metadata\n",
        "from libxmp import *         # metadata\n",
        "import libxmp                # metadata\n",
        "import boto3\n",
        "import json\n",
        "import gc\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "sys.path.append('./CLIP')\n",
        "\n",
        "import clip\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        " \n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        " \n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        " \n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        " \n",
        "    input = input.view([n * c, 1, h, w])\n",
        " \n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        " \n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        " \n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "\n",
        "class ReplaceGrad(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x_forward, x_backward):\n",
        "        ctx.shape = x_backward.shape\n",
        "        return x_forward\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        return None, grad_in.sum_to_size(ctx.shape)\n",
        " \n",
        "replace_grad = ReplaceGrad.apply\n",
        " \n",
        "class ClampWithGrad(torch.autograd.Function):\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, min, max):\n",
        "        ctx.min = min\n",
        "        ctx.max = max\n",
        "        ctx.save_for_backward(input)\n",
        "        return input.clamp(min, max)\n",
        " \n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_in):\n",
        "        input, = ctx.saved_tensors\n",
        "        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\n",
        " \n",
        "clamp_with_grad = ClampWithGrad.apply\n",
        " \n",
        "def vector_quantize(x, codebook):\n",
        "    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n",
        "    indices = d.argmin(-1)\n",
        "    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n",
        "    return replace_grad(x_q, x)\n",
        " \n",
        "class Prompt(nn.Module):\n",
        "\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        " \n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean() \n",
        " \n",
        "class MakeCutouts(nn.Module):\n",
        "\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "        self.augs = nn.Sequential(\n",
        "            K.RandomHorizontalFlip(p=0.5),\n",
        "            # K.RandomSolarize(0.01, 0.01, p=0.7),\n",
        "            K.RandomSharpness(0.3,p=0.4),\n",
        "            K.RandomAffine(degrees=30, translate=0.1, p=0.8, padding_mode='border'),\n",
        "            K.RandomPerspective(0.2,p=0.4),\n",
        "            K.ColorJitter(hue=0.01, saturation=0.01, p=0.7))\n",
        "        self.noise_fac = 0.1\n",
        " \n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)))\n",
        "        batch = self.augs(torch.cat(cutouts, dim=0))\n",
        "        if self.noise_fac:\n",
        "            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n",
        "            batch = batch + facs * torch.randn_like(batch)\n",
        "        return batch\n",
        " \n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "def resize_image(image, out_size):\n",
        "    ratio = image.size[0] / image.size[1]\n",
        "    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n",
        "    size = round((area * ratio)**0.5), round((area / ratio)**0.5)\n",
        "    return image.resize(size, Image.LANCZOS)\n",
        "\n",
        "def fetch_image(s3, job_dir, key):\n",
        "    fetched_image_dir = job_dir + \"/_images/\" \n",
        "    if not os.path.exists(fetched_image_dir):\n",
        "        os.mkdir(fetched_image_dir)\n",
        "    local_file_name = key.replace(\"/\", \"_\")\n",
        "    local_path = fetched_image_dir + \"/\" + local_file_name\n",
        "    if not os.path.exists(local_path):\n",
        "        s3.Bucket(S3_BUCKET_NAME).download_file(key, local_path)\n",
        "    return local_path\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))\n",
        "        return torch.cat(cutouts)\n",
        "\n",
        "def spherical_dist_loss(x, y):\n",
        "    x = F.normalize(x, dim=-1)\n",
        "    y = F.normalize(y, dim=-1)\n",
        "    return (x - y).norm(dim=-1).div(2).arcsin().pow(2).mul(2)\n",
        "\n",
        "def tv_loss(input):\n",
        "    \"\"\"L2 total variation loss, as in Mahendran et al.\"\"\"\n",
        "    input = F.pad(input, (0, 1, 0, 1), 'replicate')\n",
        "    x_diff = input[..., :-1, 1:] - input[..., :-1, :-1]\n",
        "    y_diff = input[..., 1:, :-1] - input[..., :-1, :-1]\n",
        "    return (x_diff**2 + y_diff**2).mean([1, 2, 3])\n",
        "\n",
        "\n",
        "model_names={\n",
        "    \"vqgan_imagenet_f16_16384\" : 'ImageNet 16384',\n",
        "    \"vqgan_imagenet_f16_1024\" : \"ImageNet 1024\",\n",
        "    \"wikiart_1024\" : \"WikiArt 1024\",\n",
        "    \"wikiart_16384\" : \"WikiArt 16384\",\n",
        "    \"coco\" : \"COCO-Stuff\",\n",
        "    \"faceshq\" : \"FacesHQ\",\n",
        "    \"sflckr\" : \"S-FLCKR\"\n",
        "}\n",
        "\n",
        "def run_job(s3, job):\n",
        "    print('starting job:', job['slug'])\n",
        "    torch.cuda.empty_cache()\n",
        "    with torch.no_grad():\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    vqgan_model = job['vqgan_model']\n",
        "    model_name = model_names[vqgan_model]\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # Use the designated seed, or choose a random seed\n",
        "    if 'seed' in job:\n",
        "        job['seed'] = torch.seed()\n",
        "    else:\n",
        "        job['seed'] = job['seed']\n",
        "    torch.manual_seed(job['seed'])\n",
        "\n",
        "    # Make a folder for this job\n",
        "    job_dir = abs_root_path + f\"/{job['slug']}\"\n",
        "    if not os.path.exists(job_dir):\n",
        "        os.makedirs(job_dir)\n",
        "\n",
        "    # Make a folder for the steps in this job\n",
        "    steps_dir = job_dir + \"/steps\"\n",
        "    if not os.path.exists(steps_dir):\n",
        "        os.mkdir(steps_dir)\n",
        "\n",
        "    # Write this job to a JSON file and upload that file to S3 (and then delete it locally)\n",
        "    job_file = job_dir + \"/job.json\"\n",
        "    s3_job_file = f\"{job['slug']}/job.json\"\n",
        "    with open(job_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(job, f, ensure_ascii=False, indent=2)\n",
        "    s3.Bucket(S3_BUCKET_NAME).upload_file(job_file, s3_job_file)\n",
        "    os.remove(job_file)\n",
        "\n",
        "    # Load the VQGAN and CLIP models\n",
        "    vqgan_config=f\"models/{job['vqgan_model']}.yaml\"\n",
        "    vqgan_checkpoint=f\"models/{job['vqgan_model']}.ckpt\"\n",
        "    model = load_vqgan_model(vqgan_config, vqgan_checkpoint).to(device)\n",
        "    perceptor = clip.load(job['clip_model'], jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "    # Setup VQGAN parameters\n",
        "    cut_size = perceptor.visual.input_resolution\n",
        "    e_dim = model.quantize.e_dim\n",
        "    f = 2**(model.decoder.num_resolutions - 1)\n",
        "    make_cutouts = MakeCutouts(cut_size, job['vq_cutn'], cut_pow=job['vq_cutpow'])\n",
        "    n_toks = model.quantize.n_e\n",
        "    toksX, toksY = job['width'] // f, job['height'] // f\n",
        "    sideX, sideY = toksX * f, toksY * f\n",
        "    z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n",
        "    z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n",
        "\n",
        "    # Load the initial image, or use the seed to generate a random image\n",
        "    initial_image = None\n",
        "    if job['initial_image']:\n",
        "        initial_image_path = fetch_image(s3, job_dir, job['initial_image'])\n",
        "        initial_image = Image.open(initial_image_path).convert('RGB')\n",
        "        initial_image = initial_image.resize((sideX, sideY), Image.LANCZOS)\n",
        "        z, *_ = model.encode(TF.to_tensor(initial_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "    else:\n",
        "        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "        z = one_hot @ model.quantize.embedding.weight\n",
        "        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "    z_orig = z.clone()\n",
        "    z.requires_grad_(True)\n",
        "    opt = optim.Adam([z], lr=job['vq_step_size'])\n",
        "\n",
        "    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                    std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "    # Read all the prompts (both text and image) into a list of weighted embeddings\n",
        "    pMs = []\n",
        "    for prompt in job['prompts']:\n",
        "        embed = None\n",
        "        hasText = 'text' in prompt and prompt['text'] is not None\n",
        "        hasImage = 'image' in prompt and prompt['image'] is not None \n",
        "        if hasText and hasImage:\n",
        "            raise RuntimeError(\"prompt has both a 'text' (%s) and an 'image' (%s)\" % (prompt['text'], prompt['image']))\n",
        "        elif hasText:\n",
        "            embed = perceptor.encode_text(clip.tokenize(prompt['text']).to(device)).float()\n",
        "        elif hasImage:\n",
        "            target_image_path = fetch_image(s3, job_dir, job['image'])\n",
        "            img = resize_image(Image.open(target_image_path).convert('RGB'), (sideX, sideY))\n",
        "            batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n",
        "            embed = perceptor.encode_image(normalize(batch)).float()\n",
        "        \n",
        "        weight = 1.0\n",
        "        if 'weight' in prompt:\n",
        "            weight = prompt['weight']\n",
        "        \n",
        "        # TODO: what is this?\n",
        "        stop = float('-inf')\n",
        "        \n",
        "        pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "    # TODO: what is this?\n",
        "    noise_prompt_seeds = []\n",
        "    noise_prompt_weights = []\n",
        "    for seed, weight in zip(noise_prompt_seeds, noise_prompt_weights):\n",
        "        gen = torch.Generator().manual_seed(seed)\n",
        "        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n",
        "        pMs.append(Prompt(embed, weight).to(device))\n",
        "\n",
        "    def synth(z):\n",
        "        z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n",
        "        return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n",
        "\n",
        "    def add_xmp_data(file_name, losses):\n",
        "        image = ImgTag(filename=file_name)\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'creator', 'VQGAN+CLIP', {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'title', job['title'], {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'step', str(i), {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'model', model_name, {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'seed', str(job['seed']) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'loss', str(sum(losses).item()) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.xmp.append_array_item(libxmp.consts.XMP_NS_DC, 'losses', ', '.join(f'{loss.item():g}' for loss in losses) , {\"prop_array_is_ordered\":True, \"prop_value_is_array\":True})\n",
        "        image.close()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def checkin(i, losses):\n",
        "        losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "        tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n",
        "        out = synth(z)\n",
        "        TF.to_pil_image(out[0].cpu()).save('progress.png')\n",
        "        add_xmp_data('progress.png', losses)\n",
        "        display.display(display.Image('progress.png'))\n",
        "\n",
        "    def ascend_txt(i):\n",
        "        out = synth(z)\n",
        "        iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n",
        "\n",
        "        lossAll = []\n",
        "\n",
        "        if 'vq_init_weight' in job:\n",
        "            lossAll.append(F.mse_loss(z, z_orig) * job['vq_init_weight'] / 2)\n",
        "\n",
        "        # Measure losses for each of the prompts\n",
        "        for prompt in pMs:\n",
        "            lossAll.append(prompt(iii))\n",
        "\n",
        "        img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n",
        "        img = np.transpose(img, (1, 2, 0))\n",
        "\n",
        "        # Emit the image as a file, and add XMP data to it\n",
        "        filename = steps_dir + f\"/{i:04}.png\"\n",
        "        imageio.imwrite(filename, np.array(img))\n",
        "        add_xmp_data(filename, lossAll)\n",
        "\n",
        "        # Upload this file to S3, and then delete it locally.\n",
        "        # TODO: do this on a background thread, so the ML code doesn't have to wait for file uploads.\n",
        "        s3_filename = f\"{job['slug']}/steps/{i:04}.png\"\n",
        "        s3.Bucket(S3_BUCKET_NAME).upload_file(filename, s3_filename)\n",
        "        os.remove(filename)\n",
        "\n",
        "        # Blend this image with the initial image and then feed the result back into the evolution process\n",
        "        if initial_image and prompt['initial_blend_amount'] and prompt['initial_blend_interval']:\n",
        "            if i % prompt['initial_blend_interval'] == 0:\n",
        "                img = Image.blend(img, initial_image, prompt['initial_blend_amount'])\n",
        "                z, *_ = model.encode(TF.to_tensor(img).to(device).unsqueeze(0) * 2 - 1)\n",
        "\n",
        "        return lossAll\n",
        "\n",
        "    def train(i):\n",
        "        opt.zero_grad()\n",
        "        lossAll = ascend_txt(i)\n",
        "        if 'display_steps' in job and i in job['display_steps']:\n",
        "            checkin(i, lossAll)\n",
        "        loss = sum(lossAll)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        with torch.no_grad():\n",
        "            z.copy_(z.maximum(z_min).minimum(z_max))\n",
        "\n",
        "    i = 0\n",
        "    try:\n",
        "        with tqdm() as pbar:\n",
        "            while True:\n",
        "                train(i)\n",
        "                if i == job['steps']:\n",
        "                    break\n",
        "                i += 1\n",
        "                pbar.update()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "    except KeyboardInterrupt:\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "4lACzTFbJzuc"
      },
      "outputs": [],
      "source": [
        "\n",
        "#@markdown #Run The Jobs!\n",
        "\n",
        "os.chdir(abs_root_path)\n",
        "\n",
        "JSON_LOCAL_FILE_PATH = '/content/jobs.json'\n",
        "\n",
        "# If the JSON file already exists, remove it first, so that we can download a fresh copy\n",
        "if os.path.exists(JSON_LOCAL_FILE_PATH):\n",
        "    os.remove(JSON_LOCAL_FILE_PATH)\n",
        "\n",
        "# Download the JOBS json file from S3\n",
        "s3 = boto3.resource('s3', aws_access_key_id = AWS_ACCESS_KEY_ID, aws_secret_access_key= AWS_SECRET_ACCESS_KEY)\n",
        "s3.Bucket(S3_BUCKET_NAME).download_file(S3_JOBS_JSON_PATH, JSON_LOCAL_FILE_PATH)\n",
        "\n",
        "# Parse the JSON and iterate through the jobs\n",
        "jobs = []\n",
        "with open(JSON_LOCAL_FILE_PATH, \"r\") as read_file:\n",
        "    jobs = json.load(read_file)\n",
        "\n",
        "for job in jobs:\n",
        "    run_job(s3, job)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "VyN-gqJKfLAu"
      ],
      "include_colab_link": true,
      "machine_shape": "hm",
      "name": "S2 GAN Art Generator (VQGAN, CLIP, Guided Diffusion) ",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
